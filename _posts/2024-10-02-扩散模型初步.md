---
layout: post
title: 扩散模型初步
date: 2024-10-02 11:12:00-0400
description: 扩散模型入门，包括DDPM和SLDM
tags: diffusion-model
categories: machine-learning 
related_posts: false
---
## 目录

- [目录](#目录)
- [1 背景](#1-背景)
  - [1.1 ELBO证据下界](#11-elbo证据下界)
  - [1.2 Variational Autoencoders](#12-variational-autoencoders)
- [2 变分扩散模型](#2-变分扩散模型)


## 1 背景
### 1.1 ELBO证据下界
可以将潜在变量和观测变量视为一个联合分布 $p(\boldsymbol{x,z})$ ，一个基础的思路是使用“似然法”，是所有观测 $\boldsymbol{x}$ 的似然 $p(\boldsymbol{x})$ 最大化，有两种方法可以得到似然：

$$p(\boldsymbol{x})=\int p(\boldsymbol{x},\boldsymbol{z})\mathrm{d}\boldsymbol{z}$$

或者可以使用条件概率：

$$p(\boldsymbol{x})=\frac{p(\boldsymbol{x},\boldsymbol{z})}{p(\boldsymbol{z}\vert\boldsymbol{x})}$$

这两个式子在计算上都有很大的困难，第一个式子的问题在于需要对所有的$\boldsymbol{z}$进行积分，第二个式子的难度在于需要知道$p(\boldsymbol{z}\vert\boldsymbol{x})$，但是利用这两个公式我们可以得到ELBO，即证据下界。证据是观测数据的对数似然，最大化ELBO时，即完美情况下，ELBO与证据相等。

$$\log p(\boldsymbol{x})\geq\mathbb{E}_{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\bigg[\log\frac{p(\boldsymbol{x},\boldsymbol{z})}{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\bigg]$$

这里，$q_\phi(\boldsymbol{z}|\boldsymbol{x})$是优化参数为$\phi$的变分近似分布。是一个可参数化的模型，用来估计给定观测值$\boldsymbol{x}$在潜在变量$\boldsymbol{z}$上的真实分布。

$$\begin{aligned}
\operatorname{log}p(\boldsymbol{x})& =\int q_\phi(\boldsymbol{z}\vert\boldsymbol{x})\log p(\boldsymbol{x})\mathrm{d}\boldsymbol{z} \\
&=\mathbb{E}_{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\begin{bmatrix}\log p(\boldsymbol{x})\end{bmatrix} \\
&=\mathbb{E}_{q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x},\boldsymbol{z})q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}{p(\boldsymbol{z}\vert\boldsymbol{x})q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}\right] \\
&=\mathbb{E}_{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\Bigg[\log\frac{p(\boldsymbol{x},\boldsymbol{z})}{q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}\Bigg]-\mathbb{E}_{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\Bigg[\log\frac{p(\boldsymbol{z}\vert\boldsymbol{x})}{q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}\Bigg] \\
&=\mathbb{E}_{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\Bigg[\log\frac{p(\boldsymbol{x},\boldsymbol{z})}{q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}\Bigg]+D_{KL}(q_\phi(\boldsymbol{z}\vert\boldsymbol{x})\Vert p(\boldsymbol{z}\vert\boldsymbol{x}))
\end{aligned}$$

即证据等于证据下界ELBO与近似后验分布$q_{\phi}(\boldsymbol{z}| \boldsymbol{x})$、真实后验分布$p(\boldsymbol{z}\vert\boldsymbol{x})$的KL散度之和。

在引入潜变量之后，我们的目标是学习描述观测数据的潜在结构，即通过最小化KL散度优化变分后验分布的参数使其与真实后验分布相同。但因为无法直接得到真实后验，因此无法直接进行优化。但因为数据似然（证据）是从联合分布中求出所有的潜在变量的边缘分布得到，与$\phi$无关，因此是相对于$\phi$的常数。即ELBO与KL散度之和为常数，因此对于$\phi$的ELBO最大化，一定会得到KL散度的最小化，因此ELBO可以作为模型建立的替代目标被最大化。

### 1.2 Variational Autoencoders

在VAE中使用变分直接最大化ELBO，在参数$\phi$参数化的后验分布的潜空间中搜素最优解，被称为自编码器。

$$\begin{aligned}
&\mathbb{E}_{q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}\left[\log\frac{p(\boldsymbol{x},\boldsymbol{z})}{q_\phi(\boldsymbol{z}\vert\boldsymbol{x})}\right] \\
=&\mathbb{E}_{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\left[\log\frac{p_\theta(\boldsymbol{x}|\boldsymbol{z})p(\boldsymbol{z})}{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\right] \\
=&\underbrace{\mathbb{E}_{q_\phi(\boldsymbol{z}|\boldsymbol{x})}\left[\log p_\theta(\boldsymbol{x}|\boldsymbol{z})\right]}_{\text{重建项}}-\underbrace{D_{KL}(q_\phi(\boldsymbol{z}\vert\boldsymbol{x})\parallel p(\boldsymbol{z}))}_{\text{先验匹配项}}
\end{aligned}$$

这种情况下，学习一个编码器$q_\phi(\boldsymbol{z}\vert\boldsymbol{x})$，将输入转换为潜在变量上的分布，同时学习一个函数$p_\theta(\boldsymbol{x}|\boldsymbol{z})$将给定潜在变量转换为观测值，即解码器。

第一项是解码器重建似然性，第二项是变分分布和先验分布在潜在变量空间上的相似度，最大化ELBO相当于最大化第一项，并最小化第二项。

VAE的一个特点是联合优化两个参数$\phi$和$\theta$，先验通常被选择为标准高斯分布：

$$\begin{aligned}q_\phi(\boldsymbol{z}\mid\boldsymbol{x})&=\mathcal{N}(\boldsymbol{z};\boldsymbol{\mu}_\phi(\boldsymbol{x}),\boldsymbol{\sigma}_\phi(\boldsymbol{x})\boldsymbol{I})\\p(\boldsymbol{z})&=\mathcal{N}(\boldsymbol{z};0,\boldsymbol{I})\end{aligned}$$

由此，ELBO的KL散度可以被解析计算，重构项（第一项）可以使用Monte Carlo方法近似：

$$\begin{aligned}&\arg\max_{\phi,\theta}\mathbb{E}_{q_{\phi}(\boldsymbol{z}|\boldsymbol{x})}\left\lfloor\log p_{\theta}(\boldsymbol{x}|\boldsymbol{z})\right\rfloor-D_{KL}(q_{\phi}(\boldsymbol{z}|\boldsymbol{x})\parallel p(\boldsymbol{z})) \\
\approx&\arg\max_{\phi,\theta}\frac{1}{I}\sum_{I}^{L}\log p_{\theta}(\boldsymbol{x} | \boldsymbol{z}^{(l)})-D_{KL}(q_{\phi}(\boldsymbol{z} | \boldsymbol{x})\| p(\boldsymbol{z}))\end{aligned}$$

我们从每个数据集的观察值使用$q_\phi(\boldsymbol{z}\vert\boldsymbol{x})$对潜变量进行采样，但我们计算损失的每个值都是随机采样的，会导致不可导。使用重参数化可以解决这个问题，重参数化将一个随机变量写为一个噪声变量的确定的函数，由此可以使用梯度下降优化非随机项，例如正态分布中采样的样本满足：

$$\boldsymbol{x}=\boldsymbol{\mu}+\boldsymbol{\sigma}\boldsymbol{\epsilon}\quad\mathrm{with}\quad \boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{\epsilon};0,\boldsymbol{I})$$

在VAE中，每个$\boldsymbol{z}$都被计算为输入$\boldsymbol{x}$和噪声变量$\boldsymbol{\epsilon}$的确定性函数：

$$\boldsymbol{z}=\boldsymbol{\mu}_\phi(\boldsymbol{x})+\boldsymbol{\sigma}_\phi(\boldsymbol{x})\odot\boldsymbol{\epsilon} \quad with\quad \boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{\epsilon};0,\boldsymbol{I})$$

其中$\odot$表示逐个元素相乘，在重参数化的情况下，梯度可以根据$\phi$计算，由此优化$\boldsymbol{\mu}_\phi$和
$\boldsymbol{\sigma}_\phi$，VAE利用重参数化和Monte Carlo来优化ELBO。

训练完成之后，可以直接从潜空间采样，并将其输入解码器生成。潜变量维度小于样本维度时，会有比较好的结果。

## 2 变分扩散模型

变分扩散模型（Variational Diffusion Model，VDM）最简单的理解是一个具有三个关键限制的马尔可夫层级变分自编码器：

- 潜在维度与数据维度完全相等。
- 每个时刻潜在编码器的结构不是学习得来的，而是预定义为线性高斯模型。
- 潜在编码器的高斯参数随时间变化，使得最终时刻的潜在分布是标准高斯分布。

根据第一个限制，我们可以得到将真实数据样本和潜变量多都表示为$\boldsymbol{x}_{t}$，其中$t=0$时表示真实数据，$t\in[1,T]$，表示相应的具有由$t$索引的层级的潜变量。

$$q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_0)=\prod_{t=1}^Tq(\boldsymbol{x}_t\mid\boldsymbol{x}_{t-1})$$

根据第二个假设，编码器中每个潜变量分布是围绕前一个潜变量的高斯分布，编码器不会被学习，而是被固定，均值和标准差可以事先设置为超参数或者学习的参数：

$$q(\boldsymbol{x}_t\mid\boldsymbol{x}_{t-1})=\mathcal{N}(\boldsymbol{x}_t;\sqrt{\alpha_t}\boldsymbol{x}_{t-1},(1-\alpha_t)\boldsymbol{I})$$

根据第三个假设，最终潜变量分布是一个标准高斯分布：

$$p(\boldsymbol{x}_{0:T})=p(\boldsymbol{x}_T)\prod_{t=1}^Tp_\theta(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t)\quad \text{where} \quad p(\boldsymbol{x}_T)=\mathcal{N}(\boldsymbol{x}_T;0,\boldsymbol{I})$$

编码器分布$q$不再由$\phi$参数化，因此我们只需要关心参数$\theta$，来学习反向过程的条件概率。

使用最大化证据下界ELBO来进行优化：

$$\begin{aligned}\log p(\boldsymbol{x})&=\log\int\frac{p(\boldsymbol{x}_{0:T})q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_{0})}{q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_{0})}\mathrm{d}\boldsymbol{x}_{1:T}\\&=\log\mathbb{E}_{q(\boldsymbol{x}_{1T}\mid\boldsymbol{x}_{0})}\left[\frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_{0})}\right]\\&\geq\mathbb{E}_{q(\boldsymbol{x}_{1T}\mid\boldsymbol{x}_{0})}\left[\log\frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}\mid\boldsymbol{x}_{0})}\right]\end{aligned}$$

更进一步我们可以得到：

$$\begin{aligned}
&\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\bigg\lfloor\log\frac{p(\boldsymbol{x}_{0:T})}{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\bigg\rfloor \\
&=\mathbb{E}_{q(\boldsymbol{x}_{1:T}|\boldsymbol{x}_0)}\Bigg[\log\frac{p(\boldsymbol{x}_T)\prod_{t=1}^Tp_\theta(\boldsymbol{x}_{t-1}\mid\boldsymbol{x}_t)}{\prod_{t=1}^Tq(\boldsymbol{x}_t\mid\boldsymbol{x}_{t-1})}\Bigg] \\
&=\mathbb{E}_{q(\boldsymbol{x}_{127}|\boldsymbol{x}_0)}\left[\log\frac{p(\boldsymbol{x}_T)p_\theta(\boldsymbol{x}_0\mid\boldsymbol{x}_1)\prod_{t=1}^{T-1}p_\theta(\boldsymbol{x}_t\mid\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_T\mid\boldsymbol{x}_{T-1})\prod_{t=1}^{T-1}q(\boldsymbol{x}_t\mid\boldsymbol{x}_{t-1})}\right] \\
&=\mathbb{E}_{q(\boldsymbol{x}_{2T}|\boldsymbol{x}_0)}\Big[\log p_\theta(\boldsymbol{x}_0\mid\boldsymbol{x}_1)\Big]+\mathbb{E}_{q(\boldsymbol{x}_{2T}|\boldsymbol{x}_0)}\Bigg[\log\frac{p(\boldsymbol{x}_T)}{q(\boldsymbol{x}_T\mid\boldsymbol{x}_{T-1})}\Bigg]\\&+\sum_{t=1}^{T-1}\mathbb{E}_{q(\boldsymbol{x}_{2T}|\boldsymbol{x}_0)}\Bigg[\log\frac{p_\theta(\boldsymbol{x}_t\mid\boldsymbol{x}_{t+1})}{q(\boldsymbol{x}_t\mid\boldsymbol{x}_{t-1})}\Bigg] \\
&=\underbrace{\mathbb{E}_{q(\boldsymbol{x}_1|\boldsymbol{x}_0)}\left[\log p_\theta(\boldsymbol{x}_0\mid\boldsymbol{x}_1)\right]}_{\text{重构项}}\\&-\underbrace{\mathbb{E}_{q(\boldsymbol{x}_{T-1}|\boldsymbol{x}_0)}\left[D_{KL}(q(\boldsymbol{x}_T\mid\boldsymbol{x}_{T-1})\|p(\boldsymbol{x}_T))\right]}_{\text{先验匹配项}}\\&-\underbrace{\sum_{t=1}^{T-1}\mathbb{E}_{q(\boldsymbol{x}_{t-1},\boldsymbol{x}_{t+1}|\boldsymbol{x}_0)}\left[D_{KL}(q(\boldsymbol{x}_t\mid\boldsymbol{x}_{t-1})\|p_\theta(\boldsymbol{x}_t\mid\boldsymbol{x}_{t+1}))\right]}_{\text{一致性项}}
\end{aligned}$$

- 第一项是重构项，预测了给定第一步潜变量时原始数据的对数概率。
- 第二项是先验匹配项，最终潜变量分布和高斯分布相匹配时，可以被最小化。这一项自然最小化，而不需要进行优化。
- 第三项是一致性项，要求每个中间时间步上，从更多噪声的图像进行去噪处理，从而与更清晰的图像进行加噪处理的结果匹配，即向前和向后的一致性。

现在出现了一个问题：在每一个时间步上都需要训练一次第三项，而为了保证第二项最小化，我们需要足够多的时间步，因此会导致训练时间成本极高。


