---
layout: post
title: 扩散模型初步
date: 2024-10-02 11:12:00-0400
description: 扩散模型入门，包括DDPM和SLDM
tags: diffusion-model
categories: machine-learning 
related_posts: false
---
## 目录

- [目录](#目录)
- [1 背景](#1-背景)
  - [1.1 ELBO证据下界](#11-elbo证据下界)
  - [1.2 Variational Autoencoders](#12-variational-autoencoders)


## 1 背景
### 1.1 ELBO证据下界
可以将潜在变量和观测变量视为一个联合分布 $p(\mathbf{x,z})$ ，一个基础的思路是使用“似然法”，是所有观测 $\mathbf{x}$ 的似然 $p(\mathbf{x})$ 最大化，有两种方法可以得到似然：

$$p(\mathbf{x})=\int p(\mathbf{x},\mathbf{z})\mathrm{d}\mathbf{z}$$

或者可以使用条件概率：

$$p(\mathbf{x})=\frac{p(\mathbf{x},\mathbf{z})}{p(\mathbf{z}\vert\mathbf{x})}$$

这两个式子在计算上都有很大的困难，第一个式子的问题在于需要对所有的$\mathbf{z}$进行积分，第二个式子的难度在于需要知道$p(\mathbf{z}\vert\mathbf{x})$，但是利用这两个公式我们可以得到ELBO，即证据下界。证据是观测数据的对数似然，最大化ELBO时，即完美情况下，ELBO与证据相等。

$$\log p(\mathbf{x})\geq\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\bigg[\log\frac{p(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}\bigg]$$

这里，$q_\phi(\mathbf{z}|\mathbf{x})$是优化参数为$\phi$的变分近似分布。是一个可参数化的模型，用来估计给定观测值$\mathbf{x}$在潜在变量$\mathbf{z}$上的真实分布。

$$\begin{aligned}
\operatorname{log}p(\mathbf{x})& =\int q_\phi(\mathbf{z}\vert\mathbf{x})\log p(\mathbf{x})\mathrm{d}\mathbf{z} \\
&=\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\begin{bmatrix}\log p(\mathbf{x})\end{bmatrix} \\
&=\mathbb{E}_{q_\phi(\mathbf{z}\vert\mathbf{x})}\left[\log\frac{p(\mathbf{x},\mathbf{z})q_\phi(\mathbf{z}\vert\mathbf{x})}{p(\mathbf{z}\vert\mathbf{x})q_\phi(\mathbf{z}\vert\mathbf{x})}\right] \\
&=\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\Bigg[\log\frac{p(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\Bigg]-\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\Bigg[\log\frac{p(\mathbf{z}\vert\mathbf{x})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\Bigg] \\
&=\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\Bigg[\log\frac{p(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\Bigg]+D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\Vert p(\mathbf{z}\vert\mathbf{x}))
\end{aligned}$$

即证据等于证据下界ELBO与近似后验分布$q_{\phi}(\mathbf{z}| \mathbf{x})$、真实后验分布$p(\mathbf{z}\vert\mathbf{x})$的KL散度之和。

在引入潜变量之后，我们的目标是学习描述观测数据的潜在结构，即通过最小化KL散度优化变分后验分布的参数使其与真实后验分布相同。但因为无法直接得到真实后验，因此无法直接进行优化。但因为数据似然（证据）是从联合分布中求出所有的潜在变量的边缘分布得到，与$\phi$无关，因此是相对于$\phi$的常数。即ELBO与KL散度之和为常数，因此对于$\phi$的ELBO最大化，一定会得到KL散度的最小化，因此ELBO可以作为模型建立的替代目标被最大化。

### 1.2 Variational Autoencoders

在VAE中使用变分直接最大化ELBO，在参数$\phi$参数化的后验分布的潜空间中搜素最优解，被称为自编码器。

$$\begin{aligned}
&\mathbb{E}_{q_\phi(\mathbf{z}\vert\mathbf{x})}\left[\log\frac{p(\mathbf{x},\mathbf{z})}{q_\phi(\mathbf{z}\vert\mathbf{x})}\right] \\
=&\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\left[\log\frac{p_\theta(\mathbf{x}|\mathbf{z})p(\mathbf{z})}{q_\phi(\mathbf{z}|\mathbf{x})}\right] \\
=&\underbrace{\mathbb{E}_{q_\phi(\mathbf{z}|\mathbf{x})}\left[\log p_\theta(\mathbf{x}|\mathbf{z})\right]}_{\text{重建项}}-\underbrace{D_{KL}(q_\phi(\mathbf{z}\vert\mathbf{x})\parallel p(\mathbf{z}))}_{\text{先验匹配项}}
\end{aligned}$$

这种情况下，学习一个编码器$q_\phi(\mathbf{z}\vert\mathbf{x})$，将输入转换为潜在变量上的分布，同时学习一个函数$p_\theta(\mathbf{x}|\mathbf{z})$将给定潜在变量转换为观测值，即解码器。

第一项是解码器重建似然性，第二项是变分分布和先验分布在潜在变量空间上的相似度，最大化ELBO相当于最大化第一项，并最小化第二项。

VAE的一个特点是联合优化两个参数$\phi$和$\theta$，先验通常被选择为标准高斯分布：

$$\begin{aligned}q_\phi(\mathbf{z}\mid\mathbf{x})&=\mathcal{N}(\mathbf{z};\mathbf{\mu}_\phi(\mathbf{x}),\mathbf{\sigma}_\phi(\mathbf{x})\mathbf{I})\\p(\mathbf{z})&=\mathcal{N}(\mathbf{z};0,\mathbf{I})\end{aligned}$$

由此，ELBO的KL散度可以被解析计算，重构项（第一项）可以使用Monte Carlo方法近似：

$$\begin{aligned}&\arg\max_{\phi,\theta}\mathbb{E}_{q_{\phi}(\mathbf{z}|\mathbf{x})}\left\lfloor\log p_{\theta}(\mathbf{x}|\mathbf{z})\right\rfloor-D_{KL}(q_{\phi}(\mathbf{z}|\mathbf{x})\parallel p(\mathbf{z})) \\
\approx&\arg\max_{\phi,\theta}\frac{1}{I}\sum_{I}^{L}\log p_{\theta}(\mathbf{x} | \mathbf{z}^{(l)})-D_{KL}(q_{\phi}(\mathbf{z} | \mathbf{x})\| p(\mathbf{z}))\end{aligned}$$

我们从每个数据集的观察值使用$q_\phi(\mathbf{z}\vert\mathbf{x})$对潜变量进行采样，但我们计算损失的每个值都是随机采样的，会导致不可导。使用重参数化可以解决这个问题，重参数化将一个随机变量写为一个噪声变量的确定的函数，由此可以使用梯度下降优化非随机项，例如正态分布中采样的样本满足：

$$\mathbf{x}=\mathbf{\mu}+\mathbf{\sigma}\boldsymbol{\epsilon}\quad\mathrm{with}\quad \boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{\epsilon};0,\mathbf{I})$$

在VAE中，每个$\boldsymbol{z}$都被计算为输入$\boldsymbol{x}$和噪声变量$\boldsymbol{\epsilon}$的确定性函数：

$$\mathbf{z}=\mathbf{\mu}_\phi(\mathbf{x})+\mathbf{\sigma}_\phi(\mathbf{x})\odot\boldsymbol{\epsilon} \quad with\quad \boldsymbol{\epsilon}\sim\mathcal{N}(\boldsymbol{\epsilon};0,\mathbf{I})$$




